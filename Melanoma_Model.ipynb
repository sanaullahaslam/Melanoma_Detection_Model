{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63dace9e-bcba-4e9d-9e26-3e1ea1e47900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1+cu121\n",
      "True\n",
      "NVIDIA GeForce RTX 2080 SUPER\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)  # Should NOT have \"+cpu\"\n",
    "print(torch.cuda.is_available())  # Should return True\n",
    "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15a32646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a simple model\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.linear = nn.Linear(10, 1)  # Simple linear layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SimpleModel().to(device)\n",
    "\n",
    "# Print model details\n",
    "print(\"Model is on:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f553f6a0-9be0-46d7-aa6f-ed4653307984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\python38\\lib\\site-packages (2.13.0)\n",
      "Requirement already satisfied: keras in c:\\python38\\lib\\site-packages (2.13.1)\n",
      "Requirement already satisfied: numpy in c:\\python38\\lib\\site-packages (1.24.3)\n",
      "Requirement already satisfied: pandas in c:\\python38\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: matplotlib in c:\\python38\\lib\\site-packages (3.7.5)\n",
      "Requirement already satisfied: opencv-python in c:\\python38\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: scikit-learn in c:\\python38\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: albumentations in c:\\python38\\lib\\site-packages (1.3.1)\n",
      "Requirement already satisfied: einops in c:\\python38\\lib\\site-packages (0.8.1)\n",
      "Requirement already satisfied: tensorflow-intel==2.13.0 in c:\\python38\\lib\\site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\python38\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\python38\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in c:\\python38\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\python38\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\python38\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\python38\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\python38\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\python38\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\python38\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\python38\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (4.25.6)\n",
      "Requirement already satisfied: setuptools in c:\\python38\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (75.3.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\python38\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\python38\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in c:\\python38\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (4.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\python38\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\python38\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.70.0)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in c:\\python38\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in c:\\python38\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\python38\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\python38\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python38\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\python38\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\python38\\lib\\site-packages (from matplotlib) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\python38\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\python38\\lib\\site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\python38\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\python38\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\python38\\lib\\site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\python38\\lib\\site-packages (from matplotlib) (6.4.5)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\python38\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\python38\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\python38\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: scikit-image>=0.16.1 in c:\\python38\\lib\\site-packages (from albumentations) (0.21.0)\n",
      "Requirement already satisfied: PyYAML in c:\\python38\\lib\\site-packages (from albumentations) (6.0.2)\n",
      "Requirement already satisfied: qudida>=0.0.4 in c:\\python38\\lib\\site-packages (from albumentations) (0.0.4)\n",
      "Requirement already satisfied: opencv-python-headless>=4.1.1 in c:\\python38\\lib\\site-packages (from albumentations) (4.11.0.86)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\python38\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib) (3.20.2)\n",
      "Requirement already satisfied: networkx>=2.8 in c:\\python38\\lib\\site-packages (from scikit-image>=0.16.1->albumentations) (3.1)\n",
      "Requirement already satisfied: imageio>=2.27 in c:\\python38\\lib\\site-packages (from scikit-image>=0.16.1->albumentations) (2.35.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in c:\\python38\\lib\\site-packages (from scikit-image>=0.16.1->albumentations) (2023.7.10)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in c:\\python38\\lib\\site-packages (from scikit-image>=0.16.1->albumentations) (1.4.1)\n",
      "Requirement already satisfied: lazy_loader>=0.2 in c:\\python38\\lib\\site-packages (from scikit-image>=0.16.1->albumentations) (0.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\python38\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.13.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\python38\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.38.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\python38\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\python38\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\python38\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\python38\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\python38\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.0.6)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\python38\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\python38\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\python38\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\python38\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\python38\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (8.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python38\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python38\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python38\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python38\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\python38\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\python38\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\python38\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries if not already installed\n",
    "!pip install tensorflow keras numpy pandas matplotlib opencv-python scikit-learn albumentations einops\n",
    "\n",
    "# Importing essential libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from einops import rearrange, repeat  # For Vision Transformer operations\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1889716c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset path\n",
    "#dataset_path = r\"E:\\21F-52 FYP Project\\dataset\"\n",
    "\n",
    "# Correct file name\n",
    "#csv_file = os.path.join(dataset_path, \"ISIC_2020_Test_Metadata.csv\")  # Ensure this name is correct\n",
    "\n",
    "# Check if the file exists\n",
    "#if os.path.exists(csv_file):\n",
    " #   print(\"✅ File found! Loading CSV...\")\n",
    "  #  df = pd.read_csv(csv_file)  # Use read_csv for CSV files\n",
    "   # print(df.head())  # Display first few rows\n",
    "#else:\n",
    " #   print(\"❌ File not found! Check the filename and extension.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set dataset path (Update this path according to your system)\n",
    "dataset_path = r\"E:\\21F-52 FYP Project\\dataset\"\n",
    "\n",
    "# Load CSV file (Update the filename if necessary)\n",
    "df = pd.read_csv(os.path.join(dataset_path, \"ISIC_2020_Training_GroundTruth.csv\"))\n",
    "\n",
    "# Extract image names and labels (Update column name if necessary)\n",
    "image_names = df[\"image_name\"].values  # Corrected column name\n",
    "labels = df[\"target\"].values  # Corrected column name\n",
    "\n",
    "# Convert labels to categorical (One-Hot Encoding for binary classification)\n",
    "labels = to_categorical(labels, num_classes=2)\n",
    "\n",
    "# Function to load and preprocess images\n",
    "def load_image(image_name, img_size=224):\n",
    "    image_path = os.path.join(dataset_path, \"train\", image_name + \".jpg\")\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"Error loading image: {image_name}\")  # Debugging\n",
    "        return np.zeros((img_size, img_size, 3))  # Return blank image if not found\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "    img = cv2.resize(img, (img_size, img_size))  # Resize for ViT\n",
    "    img = img / 255.0  # Normalize\n",
    "    return img\n",
    "\n",
    "# Load all images into a NumPy array\n",
    "images = np.array([load_image(name) for name in image_names])\n",
    "\n",
    "# Split dataset into training (80%) and validation (20%)\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to NumPy arrays (Ensures compatibility with TensorFlow)\n",
    "train_images = np.array(train_images)\n",
    "val_images = np.array(val_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "799c93f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_hub'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m layers\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhub\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Load pretrained ViT model from TensorFlow Hub\u001b[39;00m\n\u001b[0;32m      7\u001b[0m vit_model \u001b[38;5;241m=\u001b[39m hub\u001b[38;5;241m.\u001b[39mKerasLayer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://tfhub.dev/google/vit-base-patch16-224/1\u001b[39m\u001b[38;5;124m\"\u001b[39m, trainable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_hub'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# Load pretrained ViT model from TensorFlow Hub\n",
    "vit_model = hub.KerasLayer(\"https://tfhub.dev/google/vit-base-patch16-224/1\", trainable=True)\n",
    "\n",
    "# Build the model\n",
    "inputs = layers.Input(shape=(224, 224, 3))  # Input shape matches preprocessed images\n",
    "x = vit_model(inputs)  # Pass images through ViT\n",
    "x = layers.Dense(512, activation=\"relu\")(x)  # Fully connected layer\n",
    "x = layers.Dropout(0.3)(x)  # Regularization\n",
    "x = layers.Dense(2, activation=\"softmax\")(x)  # Output layer (2 classes: benign/malignant)\n",
    "\n",
    "# Compile model\n",
    "model = keras.Model(inputs, x)\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87c32c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)             (None, 14, 14, 64)           49216     ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization (Layer  (None, 14, 14, 64)           128       ['conv2d[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 12544)                0         ['layer_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " tf.reshape (TFOpLambda)     (None, 196, 64)              0         ['flatten[0][0]']             \n",
      "                                                                                                  \n",
      " add (Add)                   (None, 196, 64)              0         ['tf.reshape[0][0]']          \n",
      "                                                                                                  \n",
      " multi_head_attention (Mult  (None, 196, 64)              132672    ['add[0][0]',                 \n",
      " iHeadAttention)                                                     'add[0][0]']                 \n",
      "                                                                                                  \n",
      " add_1 (Add)                 (None, 196, 64)              0         ['add[0][0]',                 \n",
      "                                                                     'multi_head_attention[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_1 (Lay  (None, 196, 64)              128       ['add_1[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 196, 256)             16640     ['layer_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 196, 64)              16448     ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " add_2 (Add)                 (None, 196, 64)              0         ['layer_normalization_1[0][0]'\n",
      "                                                                    , 'dense_1[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_2 (Lay  (None, 196, 64)              128       ['add_2[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (Mu  (None, 196, 64)              132672    ['layer_normalization_2[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_2[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_3 (Add)                 (None, 196, 64)              0         ['layer_normalization_2[0][0]'\n",
      "                                                                    , 'multi_head_attention_1[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_3 (Lay  (None, 196, 64)              128       ['add_3[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 196, 256)             16640     ['layer_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 196, 64)              16448     ['dense_2[0][0]']             \n",
      "                                                                                                  \n",
      " add_4 (Add)                 (None, 196, 64)              0         ['layer_normalization_3[0][0]'\n",
      "                                                                    , 'dense_3[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_4 (Lay  (None, 196, 64)              128       ['add_4[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (Mu  (None, 196, 64)              132672    ['layer_normalization_4[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_4[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_5 (Add)                 (None, 196, 64)              0         ['layer_normalization_4[0][0]'\n",
      "                                                                    , 'multi_head_attention_2[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_5 (Lay  (None, 196, 64)              128       ['add_5[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 196, 256)             16640     ['layer_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dense_5 (Dense)             (None, 196, 64)              16448     ['dense_4[0][0]']             \n",
      "                                                                                                  \n",
      " add_6 (Add)                 (None, 196, 64)              0         ['layer_normalization_5[0][0]'\n",
      "                                                                    , 'dense_5[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_6 (Lay  (None, 196, 64)              128       ['add_6[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (Mu  (None, 196, 64)              132672    ['layer_normalization_6[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_6[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_7 (Add)                 (None, 196, 64)              0         ['layer_normalization_6[0][0]'\n",
      "                                                                    , 'multi_head_attention_3[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_7 (Lay  (None, 196, 64)              128       ['add_7[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dense_6 (Dense)             (None, 196, 256)             16640     ['layer_normalization_7[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dense_7 (Dense)             (None, 196, 64)              16448     ['dense_6[0][0]']             \n",
      "                                                                                                  \n",
      " add_8 (Add)                 (None, 196, 64)              0         ['layer_normalization_7[0][0]'\n",
      "                                                                    , 'dense_7[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_8 (Lay  (None, 196, 64)              128       ['add_8[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " global_average_pooling1d (  (None, 64)                   0         ['layer_normalization_8[0][0]'\n",
      " GlobalAveragePooling1D)                                            ]                             \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 64)                   0         ['global_average_pooling1d[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " dense_8 (Dense)             (None, 2)                    130       ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 713538 (2.72 MB)\n",
      "Trainable params: 713538 (2.72 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, Dropout, LayerNormalization, MultiHeadAttention, Add, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define Vision Transformer Model\n",
    "def VisionTransformer(img_size=224, patch_size=16, num_classes=2, d_model=64, num_heads=8, num_layers=4):\n",
    "    input_layer = Input(shape=(img_size, img_size, 3))\n",
    "\n",
    "    # Patch Embedding (Convert Image to Patches)\n",
    "    x = Conv2D(d_model, kernel_size=(patch_size, patch_size), strides=(patch_size, patch_size), padding=\"valid\")(input_layer)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Flatten()(x)  # Convert to 2D shape for transformer input\n",
    "\n",
    "    # Calculate the number of patches\n",
    "    num_patches = (img_size // patch_size) ** 2  # (224/16)² = 14x14 = 196 patches\n",
    "\n",
    "    # Corrected Position Embedding\n",
    "    position_embedding = Embedding(input_dim=num_patches, output_dim=d_model)(tf.range(num_patches))\n",
    "\n",
    "    # Reshape both tensors to (batch_size, num_patches, d_model)\n",
    "    x = tf.reshape(x, (-1, num_patches, d_model))  # Ensure correct shape\n",
    "    position_embedding = tf.reshape(position_embedding, (1, num_patches, d_model))  # Add batch dimension\n",
    "\n",
    "    # Add position embeddings\n",
    "    x = Add()([x, position_embedding])  \n",
    "\n",
    "    # Transformer Encoder Layers\n",
    "    for _ in range(num_layers):\n",
    "        # Multi-Head Self Attention\n",
    "        attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(x, x)\n",
    "        attn_output = Add()([x, attn_output])  # Residual connection\n",
    "        attn_output = LayerNormalization()(attn_output)\n",
    "\n",
    "        # Feed Forward Network (MLP)\n",
    "        ff_output = Dense(4 * d_model, activation=\"relu\")(attn_output)\n",
    "        ff_output = Dense(d_model)(ff_output)\n",
    "        x = Add()([attn_output, ff_output])  # Residual connection\n",
    "        x = LayerNormalization()(x)\n",
    "\n",
    "    # Classification Head\n",
    "    x = GlobalAveragePooling1D()(x)  # Global Pooling\n",
    "    x = Dropout(0.2)(x)\n",
    "    output_layer = Dense(num_classes, activation=\"softmax\")(x)  # Final classification layer\n",
    "\n",
    "    # Create Model\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    return model\n",
    "\n",
    "# Instantiate and compile the model\n",
    "model = VisionTransformer()\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cfadec29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 26502 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    dataset_path,  # <- Use dataset_path, NOT dataset_path + \"/train\"\n",
    "    target_size=(224, 224),\n",
    "    batch_size=16,\n",
    "    class_mode=\"input\",  # Use 'input' if no labels\n",
    "    subset=\"training\",\n",
    "    shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6db188ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_name, img_size=128):  # Reduced size\n",
    "    image_path = os.path.join(dataset_path, \"train\", image_name + \".jpg\")\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        return np.zeros((img_size, img_size, 3))  # Return blank image if missing\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (img_size, img_size))\n",
    "    img = img / 255.0  # Normalize\n",
    "    return img.astype(\"float32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b55b086",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 29.7 GiB for an array with shape (26500, 224, 224, 3) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Reduce from 16 to 8\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python38\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Python38\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:86\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Converts the given `value` to an `EagerTensor`.\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03mNote that this function could return cached copies of created constants for\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;124;03m  TypeError: if `dtype` is not compatible with the type of t.\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m     83\u001b[0m   \u001b[38;5;66;03m# Make a copy explicitly because the EagerTensor might share the underlying\u001b[39;00m\n\u001b[0;32m     84\u001b[0m   \u001b[38;5;66;03m# memory with the input array. Without this copy, users will be able to\u001b[39;00m\n\u001b[0;32m     85\u001b[0m   \u001b[38;5;66;03m# modify the EagerTensor after its creation by changing the input array.\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m   value \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, ops\u001b[38;5;241m.\u001b[39mEagerTensor):\n\u001b[0;32m     88\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m dtype:\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 29.7 GiB for an array with shape (26500, 224, 224, 3) and data type float64"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_images, train_labels,\n",
    "    validation_data=(val_images, val_labels),\n",
    "    epochs=20,\n",
    "    batch_size=8  # Reduce from 16 to 8\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e8adca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
